# Deep Learning Models in PyTorch ğŸš€

**In-depth, annotated implementations of popular deep learning models** â€“ tailored for clear understanding and optimized for performance on the M2 MacBook Air and high-performance desktop setups.

---

## What's Inside 

Each model comes with **intensive code annotations** to share insights into the architecture, training tricks, and reasoning behind design choices. Dive in to learn, adapt, and experiment!

### Models Implemented:

#### **Supervised Learning**

- **ResNet** ğŸ–¼ï¸  
  - **Dataset**: CIFAR-10  
  - Leverages ResNet to tackle image classification, with optimizations for running on the M2 MacBook Air.

- **Transformer** ğŸ”„  
  - **Architecture**: Simplified, with reduced parameters  
  - Designed to fit the M2 MacBook Airâ€™s performance, this reduced Transformer model retains the essence of the architecture without overwhelming your device.

- **RepVGG** ğŸ”¥  
  - **Dataset**: CIFAR-100  
  - Adapted for CIFAR-100 with reduced depth, lower width multipliers, and enhanced training techniques like AutoAugment, label smoothing, and mixup, achieving around **74% accuracy**.

#### **Reinforcement Learning**

- **Deep Q-Network (DQN)** ğŸ¢  
  - **Environment**: CartPole-v1  
  - Implements DQN for CartPole, avoiding convolutional layers (ideal for CartPoleâ€™s simple state space) to keep things lean and efficient.

- **Proximal Policy Optimization (PPO)** ğŸ¯  
  - **Environment**: HalfCheetah-v4  
  - **Description**: Implements PPO for continuous control in the HalfCheetah-v4 environment, leveraging efficient policy optimization techniques to achieve robust performance. Optimized for both the M2 MacBook Air and high-performance desktop setups, ensuring smooth training and evaluation processes.

#### **Unsupervised Learning**

- **AutoEncoder** ğŸ§©  
  - **Dataset**: CIFAR-10  
  - Trains an AutoEncoder for image reconstruction, showcasing a robust dimensionality reduction model on CIFAR-10.

- **Variational AutoEncoder (VAE)** ğŸ”  
  - **Dataset**: MNIST  
  - A compact VAE designed for the MNIST dataset, making it accessible for experimenting with generative modeling techniques.

#### **Self-Supervised Learning**

- **SimCLR** ğŸ”  
  - **Dataset**: Tiny ImageNet  
  - Implements SimCLRâ€™s contrastive learning framework on Tiny ImageNet. This model uses a ResNet backbone with a projection head to learn representations in an unsupervised manner, optimized for a high-performance desktop setup with an RTX 4060 GPU and 64GB of RAM.

---
